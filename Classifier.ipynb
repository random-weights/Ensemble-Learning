{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional flow:\n",
    "    1. Build a ANN\n",
    "    2. Train it on test set\n",
    "    3. Give an image as input and netowrk will predict the output.\n",
    "\n",
    "#### Ensemble Learning:\n",
    "    1. Build a ANN.\n",
    "    2. Make mulitple copies of ANN(in this example 5)\n",
    "    3. Train each ANN individual of others.\n",
    "    4. Give the same image to all copies of ANN and each ann will have its own prediction.\n",
    "    \n",
    "Suppose say there are n copies of the same network. we end up with n predictions from these n different copies for 1 image. How we make a final decision depends on the value of n.\n",
    "\n",
    "* If value of n is large, the value which most of the networks predicted is the final output.\n",
    "\n",
    "* If the value of n is small, we take a mean of these predictions and calcualte argmax to extract the class\n",
    "\n",
    "eg. for a particlar image from mnist dataset, the output from 5 different networks are:\n",
    "\n",
    "**[0.1  0.0  0.0  0.1  0.0  0.3  0.0  0.5  0.1  0.0  0.0]** argmax => predicted class = 7\n",
    "\n",
    "**[0.0  0.1  0.2  0.1  0.0  0.1  0.0  0.5  0.0  0.0  0.0]** argmax => predicted class = 7\n",
    "\n",
    "**[0.6  0.1  0.0  0.1  0.0  0.2  0.0  0.0  0.0  0.0  0.0]** argmax => predicted class = 0\n",
    "\n",
    "**[0.1  0.0  0.0  0.1  0.0  0.3  0.0  0.4  0.1  0.0  0.0]** argmax => predicted class = 7\n",
    "\n",
    "**[0.1  0.0  0.0  0.1  0.0  0.3  0.0  0.5  0.1  0.0  0.0]** argmax => predicted class = 7\n",
    "\n",
    "the mean of predictions looks like:\n",
    "\n",
    "**[0.18  0.04  0.08  0.10  0.12  0.12  0.18  0.38  0.06  0.00]** argmax => class = 7\n",
    "\n",
    "this is how the following example calculates predictions for ensemle network.\n",
    "\n",
    "Note that ensemble network need not be better than the best perfomring network in our bag of ensembles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Class for data handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self):\n",
    "        self.size = 0\n",
    "\n",
    "    def get_xdata(self,x_data_path):\n",
    "        df = pd.read_csv(x_data_path, sep=',', header=None)\n",
    "        a = np.array(df).astype(int)\n",
    "        self.size = len(df)\n",
    "        a = a.reshape(self.size,28,28)\n",
    "        self.x_data = a\n",
    "        return self.x_data\n",
    "\n",
    "    def get_ydata(self,y_data_path):\n",
    "        df = pd.read_csv(y_data_path,sep = ',',header = None)\n",
    "        b = np.array(df).astype(int)\n",
    "        b = b.reshape(len(df),10)\n",
    "        self.y_data = b\n",
    "        return self.y_data\n",
    "\n",
    "    def get_rand_batch(self,batch_size = None):\n",
    "        if batch_size is None:\n",
    "            b_size = 128\n",
    "        else:\n",
    "            b_size = batch_size\n",
    "\n",
    "        rand_indices = np.random.choice(self.size, b_size, replace=False)\n",
    "        x_batch = self.x_data[rand_indices]\n",
    "        self.x_batch = x_batch.reshape(b_size, 28, 28, 1)\n",
    "        self.y_batch = self.y_data[rand_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "train_data = Data()\n",
    "train_data.get_xdata(\"data/x_train.csv\")\n",
    "train_data.get_ydata(\"data/y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d5b2e2bcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = train_data.x_data[0].reshape(28,28)\n",
    "plt.imshow(img,cmap = 'binary')\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to plot images and  show the true label, predicted,ensembled label on the bottom\n",
    "adapted from Hvass laboratories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images,y_true_cls,y_pred_cls = None,y_ensemb_cls = None):\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "    # Adjust vertical spacing if we need to print ensemble and best-net.\n",
    "    if y_ensemb_cls is None:\n",
    "        hspace = 1\n",
    "    else:\n",
    "        hspace = 1.0\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "\n",
    "    # For each of the sub-plots.\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "\n",
    "        # There may not be enough images for all sub-plots.\n",
    "        if i < len(images):\n",
    "            # Plot image.\n",
    "            ax.imshow(images[i].reshape(28,28), cmap='binary')\n",
    "\n",
    "            # Show true and predicted classes.\n",
    "            if (y_ensemb_cls is None):\n",
    "                if (y_pred_cls is None):\n",
    "                    xlabel = \"True: {0}\".format(y_true_cls[i])\n",
    "                else:\n",
    "                    msg = \"True: {0} \\nPredicted: {1}\"\n",
    "                    xlabel = msg.format(y_true_cls[i],y_pred_cls[i])\n",
    "            else:\n",
    "                msg = \"True: {0}\\nNetwork: {1}\\nEnsemble: {2}\"\n",
    "                xlabel = msg.format(y_true_cls[i],y_pred_cls[i],y_ensemb_cls[i])\n",
    "\n",
    "            # Show the classes as the label on the x-axis.\n",
    "            ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will define our compuational graph\n",
    "\n",
    "\n",
    "![alt](https://github.com/thubatilakshmi/Ensemble-Learning/blob/master/network.JPG)\n",
    "\n",
    "Placeholders for input to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float',shape = [None,28,28,1],name = \"x\")\n",
    "y_true = tf.placeholder('float',shape = [None,10],name = \"y_true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls = tf.argmax(y_true,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### declaring initializers for kernels and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kern_init = tf.random_normal_initializer(mean = 0.0, stddev = 0.01)\n",
    "bias_init = tf.zeros_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hyper parameters of graph\n",
    "    1. The no. of convolution layers\n",
    "    2. No. of kernel units in each convolution layer\n",
    "    3. The window size of each kernels in each layer.\n",
    "    4. No biases in conv layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_kern_count = [16,32,128,10]\n",
    "# only the first 2 layers are conv2d, 128 and 10 are fully connected layers.\n",
    "kern_size = [[5,5],[7,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = tf.layers.conv2d(x,filters = ls_kern_count[0],\n",
    "                        kernel_size=kern_size[0],\n",
    "                        strides = [2,2],\n",
    "                        padding =\"same\",\n",
    "                        activation = tf.nn.relu,\n",
    "                        use_bias = False,\n",
    "                        kernel_initializer=kern_init,\n",
    "                        trainable=True,name = \"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = tf.layers.conv2d(conv1,filters=ls_kern_count[1],\n",
    "                        kernel_size=kern_size[1],\n",
    "                        strides=[2, 2], \n",
    "                        padding=\"same\", \n",
    "                        activation=tf.nn.relu, \n",
    "                        use_bias=False,\n",
    "                        kernel_initializer=kern_init, \n",
    "                        trainable=True,name = \"conv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 2 conv layers the input will be a 4 dimensional tensor, we flatten this out using tf.layers.flatten method,\n",
    "\n",
    "Good thing about using tf.layers api is that we donot need to keep track of output tensor size after each convolution layer, the layers API will take care of it.\n",
    "\n",
    "Without layers api when we are defining the dimensions of weight matrix for first fully connected layers we have to calculate the out put tensor dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_tensor = tf.layers.flatten(conv2,name=\"flat_tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = tf.layers.dense(flat_tensor,units = ls_kern_count[2],\n",
    "                    activation = tf.nn.relu,\n",
    "                    use_bias = True,\n",
    "                    kernel_initializer=kern_init,\n",
    "                    bias_initializer=bias_init,\n",
    "                    trainable=True,name = \"fc1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we are doing a classification problem the output layer will be softmax that gives probability of the respective class.\n",
    "Instead of calculating the output layer directly with softmax we will split into two steps.\n",
    "    1. We sum the weighted activation coming from 128 units of first layer. Lets call them logits.\n",
    "    2. Apply softmax activation function to logits, and we get probabilities of classes.\n",
    "   \n",
    "Logits is useful when we want to claculate the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(fc1,ls_kern_count[3],\n",
    "                        activation= None,\n",
    "                        use_bias = True,\n",
    "                        kernel_initializer=kern_init,\n",
    "                        bias_initializer=bias_init,\n",
    "                        trainable=True,name = \"logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(logits, name = \"y_pred\")\n",
    "y_pred_cls = tf.argmax(y_pred, axis = 1, name = \"y_pred_cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(y_pred_cls,y_true_cls,name = \"correct_pred\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this is the reason why we did the output layer in two steps. The loss function softmax_cross_entropy_with_logits will automatically compute softmax for us and apply loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits,labels=y_true),name = \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use proven and tested AdamOptimizer with learning rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 5 networks.\n",
    "#### We will train all networks inside a for loop-saving each network after it is trained for 10,000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep = 100)\n",
    "#you will know why we used this statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A helper function to save path for 5 networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'checkpoints/'\n",
    "def get_save_path(network_number):\n",
    "    return save_dir +'network'+str(network_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Network:  0\n",
      "Epoch:  0 loss on train:  2.300454 acc_on_train:  0.140625\n",
      "Epoch:  500 loss on train:  0.11859661 acc_on_train:  0.9609375\n",
      "Epoch:  1000 loss on train:  0.16330484 acc_on_train:  0.9375\n",
      "Epoch:  1500 loss on train:  0.08922082 acc_on_train:  0.9765625\n",
      "Epoch:  2000 loss on train:  0.058121763 acc_on_train:  0.984375\n",
      "Epoch:  2500 loss on train:  0.027224202 acc_on_train:  0.9921875\n",
      "Epoch:  3000 loss on train:  0.05781629 acc_on_train:  0.984375\n",
      "Epoch:  3500 loss on train:  0.016629977 acc_on_train:  0.9921875\n",
      "Epoch:  4000 loss on train:  0.056134503 acc_on_train:  0.9921875\n",
      "Epoch:  4500 loss on train:  0.009242544 acc_on_train:  1.0\n",
      "Epoch:  5000 loss on train:  0.017338792 acc_on_train:  1.0\n",
      "Epoch:  5500 loss on train:  0.0166752 acc_on_train:  0.9921875\n",
      "Epoch:  6000 loss on train:  0.012366456 acc_on_train:  0.9921875\n",
      "Epoch:  6500 loss on train:  0.0084449425 acc_on_train:  0.9921875\n",
      "Epoch:  7000 loss on train:  0.0024519465 acc_on_train:  1.0\n",
      "Epoch:  7500 loss on train:  0.0026851487 acc_on_train:  1.0\n",
      "Epoch:  8000 loss on train:  0.0046027154 acc_on_train:  1.0\n",
      "Epoch:  8500 loss on train:  0.0013813551 acc_on_train:  1.0\n",
      "Epoch:  9000 loss on train:  0.016962439 acc_on_train:  0.9921875\n",
      "Epoch:  9500 loss on train:  0.010362698 acc_on_train:  0.9921875\n",
      "\n",
      " Network:  1\n",
      "Epoch:  0 loss on train:  2.3036785 acc_on_train:  0.0703125\n",
      "Epoch:  500 loss on train:  0.16677636 acc_on_train:  0.9375\n",
      "Epoch:  1000 loss on train:  0.037902754 acc_on_train:  0.9921875\n",
      "Epoch:  1500 loss on train:  0.13837601 acc_on_train:  0.953125\n",
      "Epoch:  2000 loss on train:  0.044437595 acc_on_train:  0.984375\n",
      "Epoch:  2500 loss on train:  0.034953587 acc_on_train:  0.984375\n",
      "Epoch:  3000 loss on train:  0.019751094 acc_on_train:  0.9921875\n",
      "Epoch:  3500 loss on train:  0.05832506 acc_on_train:  0.9765625\n",
      "Epoch:  4000 loss on train:  0.0374419 acc_on_train:  0.9765625\n",
      "Epoch:  4500 loss on train:  0.002141554 acc_on_train:  1.0\n",
      "Epoch:  5000 loss on train:  0.0067570913 acc_on_train:  0.9921875\n",
      "Epoch:  5500 loss on train:  0.00070605846 acc_on_train:  1.0\n",
      "Epoch:  6000 loss on train:  0.0070239916 acc_on_train:  1.0\n",
      "Epoch:  6500 loss on train:  0.007429043 acc_on_train:  0.9921875\n",
      "Epoch:  7000 loss on train:  0.0045438334 acc_on_train:  1.0\n",
      "Epoch:  7500 loss on train:  0.0050595333 acc_on_train:  1.0\n",
      "Epoch:  8000 loss on train:  0.061274298 acc_on_train:  0.9921875\n",
      "Epoch:  8500 loss on train:  0.00926187 acc_on_train:  0.9921875\n",
      "Epoch:  9000 loss on train:  0.0009036945 acc_on_train:  1.0\n",
      "Epoch:  9500 loss on train:  0.0026354757 acc_on_train:  1.0\n",
      "\n",
      " Network:  2\n",
      "Epoch:  0 loss on train:  2.3013995 acc_on_train:  0.1328125\n",
      "Epoch:  500 loss on train:  0.20611118 acc_on_train:  0.921875\n",
      "Epoch:  1000 loss on train:  0.120654106 acc_on_train:  0.9375\n",
      "Epoch:  1500 loss on train:  0.040008157 acc_on_train:  0.984375\n",
      "Epoch:  2000 loss on train:  0.027296234 acc_on_train:  0.984375\n",
      "Epoch:  2500 loss on train:  0.03941697 acc_on_train:  0.9921875\n",
      "Epoch:  3000 loss on train:  0.046435755 acc_on_train:  0.984375\n",
      "Epoch:  3500 loss on train:  0.012090354 acc_on_train:  1.0\n",
      "Epoch:  4000 loss on train:  0.07280677 acc_on_train:  0.984375\n",
      "Epoch:  4500 loss on train:  0.013019191 acc_on_train:  0.9921875\n",
      "Epoch:  5000 loss on train:  0.0051717134 acc_on_train:  1.0\n",
      "Epoch:  5500 loss on train:  0.0039209803 acc_on_train:  1.0\n",
      "Epoch:  6000 loss on train:  0.007094049 acc_on_train:  1.0\n",
      "Epoch:  6500 loss on train:  0.0026175969 acc_on_train:  1.0\n",
      "Epoch:  7000 loss on train:  0.033301122 acc_on_train:  0.9921875\n",
      "Epoch:  7500 loss on train:  0.0010617047 acc_on_train:  1.0\n",
      "Epoch:  8000 loss on train:  0.01235172 acc_on_train:  1.0\n",
      "Epoch:  8500 loss on train:  0.036692224 acc_on_train:  0.9765625\n",
      "Epoch:  9000 loss on train:  0.0020378728 acc_on_train:  1.0\n",
      "Epoch:  9500 loss on train:  0.0047840755 acc_on_train:  1.0\n",
      "\n",
      " Network:  3\n",
      "Epoch:  0 loss on train:  2.300987 acc_on_train:  0.125\n",
      "Epoch:  500 loss on train:  0.1370996 acc_on_train:  0.9609375\n",
      "Epoch:  1000 loss on train:  0.050912302 acc_on_train:  0.984375\n",
      "Epoch:  1500 loss on train:  0.08438684 acc_on_train:  0.9765625\n",
      "Epoch:  2000 loss on train:  0.08234657 acc_on_train:  0.9765625\n",
      "Epoch:  2500 loss on train:  0.03135173 acc_on_train:  0.984375\n",
      "Epoch:  3000 loss on train:  0.057063285 acc_on_train:  0.984375\n",
      "Epoch:  3500 loss on train:  0.05633819 acc_on_train:  0.96875\n",
      "Epoch:  4000 loss on train:  0.012499158 acc_on_train:  0.9921875\n",
      "Epoch:  4500 loss on train:  0.02825013 acc_on_train:  0.984375\n",
      "Epoch:  5000 loss on train:  0.03477656 acc_on_train:  0.9921875\n",
      "Epoch:  5500 loss on train:  0.0053714397 acc_on_train:  1.0\n",
      "Epoch:  6000 loss on train:  0.01931417 acc_on_train:  0.9921875\n",
      "Epoch:  6500 loss on train:  0.0031772458 acc_on_train:  1.0\n",
      "Epoch:  7000 loss on train:  0.002039525 acc_on_train:  1.0\n",
      "Epoch:  7500 loss on train:  0.0015456153 acc_on_train:  1.0\n",
      "Epoch:  8000 loss on train:  0.0025319415 acc_on_train:  1.0\n",
      "Epoch:  8500 loss on train:  0.002715925 acc_on_train:  1.0\n",
      "Epoch:  9000 loss on train:  0.0021104496 acc_on_train:  1.0\n",
      "Epoch:  9500 loss on train:  0.00019453338 acc_on_train:  1.0\n",
      "\n",
      " Network:  4\n",
      "Epoch:  0 loss on train:  2.3031778 acc_on_train:  0.0703125\n",
      "Epoch:  500 loss on train:  0.12650597 acc_on_train:  0.96875\n",
      "Epoch:  1000 loss on train:  0.02137981 acc_on_train:  1.0\n",
      "Epoch:  1500 loss on train:  0.027864516 acc_on_train:  1.0\n",
      "Epoch:  2000 loss on train:  0.012281367 acc_on_train:  1.0\n",
      "Epoch:  2500 loss on train:  0.0054028137 acc_on_train:  1.0\n",
      "Epoch:  3000 loss on train:  0.011391301 acc_on_train:  1.0\n",
      "Epoch:  3500 loss on train:  0.010972213 acc_on_train:  1.0\n",
      "Epoch:  4000 loss on train:  0.028443117 acc_on_train:  0.9921875\n",
      "Epoch:  4500 loss on train:  0.02464112 acc_on_train:  0.984375\n",
      "Epoch:  5000 loss on train:  0.051466525 acc_on_train:  0.9921875\n",
      "Epoch:  5500 loss on train:  0.0069113895 acc_on_train:  1.0\n",
      "Epoch:  6000 loss on train:  0.021628048 acc_on_train:  0.984375\n",
      "Epoch:  6500 loss on train:  0.04419808 acc_on_train:  0.9921875\n",
      "Epoch:  7000 loss on train:  0.0044406326 acc_on_train:  1.0\n",
      "Epoch:  7500 loss on train:  0.0027083978 acc_on_train:  1.0\n",
      "Epoch:  8000 loss on train:  0.0076669855 acc_on_train:  0.9921875\n",
      "Epoch:  8500 loss on train:  0.0011839619 acc_on_train:  1.0\n",
      "Epoch:  9000 loss on train:  0.03559594 acc_on_train:  0.9921875\n",
      "Epoch:  9500 loss on train:  0.004608872 acc_on_train:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    #extract training data\n",
    "    train_data = Data()\n",
    "    train_data.get_xdata(\"data/x_train.csv\")\n",
    "    train_data.get_ydata(\"data/y_train.csv\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"\\n Network: \",str(i))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(10000):\n",
    "            train_data.get_rand_batch(128)\n",
    "            x_batch = train_data.x_batch\n",
    "            y_batch = train_data.y_batch\n",
    "            feed_dict = {x:x_batch,y_true:y_batch}\n",
    "            cost,accu,_ = sess.run([loss,accuracy,train],feed_dict)\n",
    "            if epoch%500 == 0:\n",
    "                print(\"Epoch: \",str(epoch),\"loss on train: \",str(cost),\"acc_on_train: \",str(accu))\n",
    "        \n",
    "        saver.save(sess = sess, save_path = get_save_path(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
